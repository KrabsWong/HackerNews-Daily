name = "hackernews-daily-export"
main = "dist/worker/index.js"
compatibility_date = "2024-12-11"
compatibility_flags = ["nodejs_compat"]

# Enable observability features
observability = { enabled = true }

# =============================================================================
# D1 Database Configuration (REQUIRED)
# =============================================================================
# 
# The system requires a D1 database for distributed task processing.
# You need to create TWO databases: one for production, one for development/testing.
#
# Step 1: Create production database
#   wrangler d1 create hackernews-daily-db
#   
# Step 2: Create development/testing database
#   wrangler d1 create hackernews-daily-db-dev
#
# Step 3: Initialize both databases with schema
#   wrangler d1 execute hackernews-daily-db --file=./migrations/0001_create_tables.sql
#   wrangler d1 execute hackernews-daily-db-dev --file=./migrations/0001_create_tables.sql
#
# Step 4: Copy the database IDs from the output and paste them below
#
# IMPORTANT: The dev database is used for local development and testing.
# This ensures you never accidentally write to or corrupt production data.
# =============================================================================

# Production database binding
[[d1_databases]]
binding = "DB"
database_name = "hackernews-daily-db"
database_id = "<your-production-database-id>"  # Replace with actual ID from wrangler d1 create

# Development/Testing database binding (for local development)
# This will be used when running `wrangler dev` or `npm run dev:worker`
[[env.dev.d1_databases]]
binding = "DB"
database_name = "hackernews-daily-db-dev"
database_id = "<your-dev-database-id>"  # Replace with actual ID from wrangler d1 create

# =============================================================================
# Distributed Task Processing Configuration
# =============================================================================

[vars]
# Number of articles to process per batch (1-10, default: 6)
# Each batch consumes approximately 25 subrequests
# Free tier limit: 50 subrequests per execution
TASK_BATCH_SIZE = "6"

# Maximum retry attempts for failed articles (default: 3)
MAX_RETRY_COUNT = "3"

# Story fetching configuration
HN_STORY_LIMIT = "30"
HN_TIME_WINDOW_HOURS = "24"
SUMMARY_MAX_LENGTH = "300"

# Content filtering
ENABLE_CONTENT_FILTER = "false"
CONTENT_FILTER_SENSITIVITY = "medium"

# Cache configuration (set to "false" for workers)
CACHE_ENABLED = "false"

# LLM batch processing
LLM_BATCH_SIZE = "15"

# =============================================================================
# REQUIRED: You MUST set these variables before deploying
# =============================================================================

# LLM Provider: "deepseek" or "openrouter"
# LLM_PROVIDER = "deepseek"

# Target GitHub repository (format: "username/repo-name")
# IMPORTANT: Change this to YOUR repository to avoid pushing to the wrong place!
# TARGET_REPO = "yourusername/your-repo-name"

# Target branch (usually "main" or "master")
TARGET_BRANCH = "main"

# =============================================================================
# LLM Configuration (Optional - set in [vars] section if needed)
# =============================================================================
#
# DeepSeek model override (default: deepseek-chat)
# LLM_DEEPSEEK_MODEL = "deepseek-chat"
#
# OpenRouter model override (default: deepseek/deepseek-chat-v3-0324)
# LLM_OPENROUTER_MODEL = "deepseek/deepseek-chat-v3-0324"
#
# OpenRouter site attribution (optional)
# LLM_OPENROUTER_SITE_URL = "https://your-site.com"
# LLM_OPENROUTER_SITE_NAME = "Your Site Name"
#
# =============================================================================

# =============================================================================
# Secrets (set via: wrangler secret put SECRET_NAME)
# =============================================================================
# 
# Based on your LLM_PROVIDER choice, set ONE of:
#   - LLM_DEEPSEEK_API_KEY (if LLM_PROVIDER=deepseek)
#   - LLM_OPENROUTER_API_KEY (if LLM_PROVIDER=openrouter)
#
# Also required:
#   - GITHUB_TOKEN (GitHub personal access token with repo scope)
#
# Example commands:
#
#   For DeepSeek:
#     wrangler secret put LLM_DEEPSEEK_API_KEY
#     wrangler secret put GITHUB_TOKEN
#
#   For OpenRouter:
#     wrangler secret put LLM_OPENROUTER_API_KEY
#     wrangler secret put GITHUB_TOKEN
#
# Optional:
#   wrangler secret put CRAWLER_API_URL
#   wrangler secret put CRAWLER_API_TOKEN  # Required for private HF Spaces crawler
#
# =============================================================================

# Scheduled triggers (cron format)
# Default: Every 10 minutes for incremental processing
# The system will process one batch (6 articles) per trigger
# Total time to process 30 articles: ~50 minutes (5 batches)
[triggers]
crons = ["*/10 * * * *"]

# =============================================================================
# Environment-specific Log Levels
# =============================================================================

[env.production]
log_level = "log"

[env.dev]
log_level = "debug"
